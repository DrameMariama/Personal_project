{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Private Training by Output Perturbation.\"\"\"\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "import torch\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import nn\n",
    "\n",
    "from logistic_regression import nonprivate_logistic_regression\n",
    "from utils import get_data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gamma_sample_pytorch_parameterization(concentration, rate):\n",
    "    \"\"\"The Gamma dist'n as it is parameterized in PyTorch\"\"\"\n",
    "    return Gamma(concentration, rate).sample()\n",
    "\n",
    "\n",
    "def gamma_sample_chaudhuri_parameterization(concentration, scale):\n",
    "    \"\"\"The Gamma dist'n as it is parameterized in Chaudhuri and Monteleoni\"\"\"\n",
    "    rate = 1. / scale\n",
    "    return gamma_sample_pytorch_parameterization(concentration, rate)\n",
    "\n",
    "\n",
    "def random_unit_norm_vector(num_dims):\n",
    "    random_rotation_matrix = ortho_group.rvs(num_dims)\n",
    "    basis_vector_one = np.eye(num_dims)[0]\n",
    "    vector = np.matmul(random_rotation_matrix, basis_vector_one)\n",
    "    return torch.tensor(vector, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True/False propositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\lambda$ and $\\epsilon$ fixed, increasing n requires Algorithm 1 to add more noise to w*: False\n",
    "\n",
    "With $\\lambda$ and n fixed, increasing $\\epsilon$ requires Algorithm 1 to add more noise to w*: False\n",
    "\n",
    "With $\\epsilon$  and n fixed, increasing  $\\lambda$ requires Algorithm 1 to add more noise to w*: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def private_logistic_regression(dset_loader, num_epochs, learning_rate,\n",
    "    lmbda, epsilon, seed=None):\n",
    "    ############################################################################\n",
    "    # TODO(student)\n",
    "    #\n",
    "    # your code here...\n",
    "    #\n",
    "    # hint: use the code we have given you. For example you don't have to \n",
    "    # implement non-private logistic regression from scratch because an \n",
    "    # implementation exists in logistic_regression.py. There are also functions \n",
    "    # in this file for sampling Laplace noise\n",
    "    #\n",
    "    # hint: the input dim d can be found as a attr of the dset_loader's dset\n",
    "    #       >>> num_pixels = dset_loader.dataset.num_pixels\n",
    "    #\n",
    "    non_private_train_params = nonprivate_logistic_regression(dset_loader, num_epochs, learning_rate, lmbda, seed)\n",
    "    d = dset_loader.dataset.num_pixels\n",
    "    \n",
    "    norm_eta = gamma_sample_chaudhuri_parameterization(d, 2.0/(len(dset_loader.dataset)*epsilon*lmbda))\n",
    "    # = np.exp(-((len(dset_loader)*epsilon*lmda)/2)*norm_eta)\n",
    "    direction = random_unit_norm_vector(d)\n",
    "    \n",
    "    eta = norm_eta * direction\n",
    "    output_weight = non_private_train_params['weight'] + torch.tensor(eta)\n",
    "    private_params = {\n",
    "        'weight': torch.tensor(output_weight),  # replace me (but this is how to format the state_dict)\n",
    "        }\n",
    "    #raise NotImplementedError\n",
    "    ############################################################################\n",
    "\n",
    "    \n",
    "    return private_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(n, epsilon, lmbda, epochs, batch_size, lr, data_seed, model_seed):\n",
    "    # load data\n",
    "    loaders, _ = get_data_loaders(data_seed, batch_size, n)\n",
    "    loaders.pop('neighbor')  # don't need this loader for this question\n",
    "  \n",
    "    # train model\n",
    "    nonprivate_params = \\\n",
    "            nonprivate_logistic_regression(loaders['train'], epochs, \n",
    "                    lr, lmbda, seed=model_seed)\n",
    "  \n",
    "    private_params = private_logistic_regression(loaders['train'], epochs, \n",
    "        lr, lmbda, epsilon, seed=model_seed)\n",
    "  \n",
    "    # evaluate\n",
    "    test_losses = dict()\n",
    "    test_accs = dict()\n",
    "    for name, params in zip(['nonprivate', 'private'], \n",
    "          [nonprivate_params, private_params]):\n",
    "        num_pixels = loaders['train'].dataset.num_pixels\n",
    "        model = nn.Linear(num_pixels, 1, bias=False)\n",
    "        criterion = nn.BCEWithLogitsLoss()  # binary cross entropy\n",
    "        model.load_state_dict(params)\n",
    "        model.eval()\n",
    "        num_test_examples = len(loaders['test'].dataset)\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in loaders['test']:\n",
    "                images = images.reshape(-1, 28*28)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                test_loss += loss.item() * len(images) / float(num_test_examples)\n",
    "                predicted = (outputs.squeeze() > 0.).long()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            test_acc = float(correct) / float(total)\n",
    "            test_losses[name] = test_loss\n",
    "            test_accs[name] = 100. * test_acc  # format as a percentage\n",
    "  \n",
    "    from pprint import pprint\n",
    "    print('final test losses')\n",
    "    print('nonprivate: {nonprivate:.2f}, private: {private:.2f}'\n",
    "          .format(**test_losses))\n",
    "    print('final test accs')\n",
    "    print('nonprivate: {nonprivate:.2f}, private: {private:.2f}'\n",
    "          .format(**test_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.07it/s]\n",
      "100%|██████████| 50/50 [00:10<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.09, private: 3.08\n",
      "final test accs\n",
      "nonprivate: 97.50, private: 67.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel/__main__.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/lib/python3/dist-packages/ipykernel/__main__.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "N = 2000 \n",
    "EPSILON = 2.\n",
    "LMBDA = 5e-4\n",
    "EPOCHS = 50  # run for more epochs once your code works\n",
    "BATCH_SIZE = 256\n",
    "LR = .1\n",
    "DATA_SEED = 0\n",
    "MODEL_SEED = 0\n",
    "main(N, EPSILON, LMBDA, EPOCHS, BATCH_SIZE, LR, DATA_SEED, MODEL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments and main function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   lambda = 0.5 ===========================\n",
      "===========================   N = 1000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.77it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.92it/s]\n",
      "/usr/lib/python3/dist-packages/ipykernel/__main__.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/lib/python3/dist-packages/ipykernel/__main__.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.64, private: 0.64\n",
      "final test accs\n",
      "nonprivate: 96.00, private: 87.00\n",
      "===========================   N = 2000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  4.99it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.65, private: 0.65\n",
      "final test accs\n",
      "nonprivate: 95.00, private: 93.50\n",
      "===========================   N = 3000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.29it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.12it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.65, private: 0.64\n",
      "final test accs\n",
      "nonprivate: 93.67, private: 95.33\n",
      "===========================   N = 5000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n",
      " 10%|█         | 1/10 [00:00<00:01,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.64, private: 0.64\n",
      "final test accs\n",
      "nonprivate: 92.80, private: 93.20\n",
      "=================================================================\n",
      "===========================   lambda = 0.05 ===========================\n",
      "===========================   N = 1000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.44it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.13it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.42, private: 0.44\n",
      "final test accs\n",
      "nonprivate: 96.00, private: 89.00\n",
      "===========================   N = 2000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.92it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.43, private: 0.47\n",
      "final test accs\n",
      "nonprivate: 95.50, private: 82.00\n",
      "===========================   N = 3000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.37it/s]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.28it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.43, private: 0.44\n",
      "final test accs\n",
      "nonprivate: 93.67, private: 91.67\n",
      "===========================   N = 5000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.07it/s]\n",
      " 10%|█         | 1/10 [00:00<00:00,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.43, private: 0.43\n",
      "final test accs\n",
      "nonprivate: 94.60, private: 94.60\n",
      "=================================================================\n",
      "===========================   lambda = 0.005 ===========================\n",
      "===========================   N = 1000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00,  9.99it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 10.01it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.32, private: 4.11\n",
      "final test accs\n",
      "nonprivate: 96.00, private: 34.00\n",
      "===========================   N = 2000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.92it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.90it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.22, private: 1.09\n",
      "final test accs\n",
      "nonprivate: 97.00, private: 52.50\n",
      "===========================   N = 3000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.34it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.37it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.20, private: 0.26\n",
      "final test accs\n",
      "nonprivate: 95.33, private: 88.00\n",
      "===========================   N = 5000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      " 10%|█         | 1/10 [00:00<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.19, private: 0.13\n",
      "final test accs\n",
      "nonprivate: 95.80, private: 96.20\n",
      "=================================================================\n",
      "===========================   lambda = 0.0005 ===========================\n",
      "===========================   N = 1000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.75it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.86it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.31, private: 5.09\n",
      "final test accs\n",
      "nonprivate: 96.00, private: 79.00\n",
      "===========================   N = 2000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.98it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.19, private: 5.90\n",
      "final test accs\n",
      "nonprivate: 97.00, private: 60.50\n",
      "===========================   N = 3000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.32it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.34it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.17, private: 6.18\n",
      "final test accs\n",
      "nonprivate: 95.33, private: 44.67\n",
      "===========================   N = 5000 ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.13, private: 1.56\n",
      "final test accs\n",
      "nonprivate: 96.00, private: 66.20\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "#N = 3000 \n",
    "EPSILON = 2.\n",
    "#LMBDA = 5e-3\n",
    "EPOCHS = 10  # run for more epochs once your code works\n",
    "BATCH_SIZE = 256\n",
    "LR = .1\n",
    "DATA_SEED = 0\n",
    "MODEL_SEED = 0\n",
    "N = [1000, 2000, 3000, 5000]\n",
    "lmbda = [5e-1, 5e-2, 5e-3, 5e-4]\n",
    "for l in lmbda:\n",
    "    print('===========================   lambda = '+str(l)+' ===========================')\n",
    "    for n in N:\n",
    "        print('===========================   N = '+str(n)+' ===========================')\n",
    "        main(n, EPSILON, l, EPOCHS, BATCH_SIZE, LR, DATA_SEED, MODEL_SEED)\n",
    "    print('=================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
